{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation using Theano with Lasagne. However, it is planned to be replaced with tensorflow/keras implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import random \n",
    "from PIL import Image\n",
    "import sys, os\n",
    "import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu        = lasagne.nonlinearities.rectify\n",
    "lrelu       = lasagne.nonlinearities.LeakyRectify(0.2)\n",
    "tanh        = lasagne.nonlinearities.tanh\n",
    "sigmoid     = lasagne.nonlinearities.sigmoid\n",
    "conv        = lambda incoming, num_filters, filter_size, W, b, nonlinearity: \\\n",
    "                lasagne.layers.Conv2DLayer(incoming, num_filters, filter_size, stride=(2,2), pad='same', W=W, b=b, flip_filters=True, nonlinearity=nonlinearity)\n",
    "tconv       = lambda incoming, num_filters, filter_size, W, nonlinearity: lasagne.layers.TransposedConv2DLayer(incoming, num_filters, filter_size, stride=(2,2), crop='same', W=W, nonlinearity=nonlinearity)\n",
    "batchnorm   = lasagne.layers.batch_norm\n",
    "\n",
    "# bias and weight initializations\n",
    "w_init      = lasagne.init.Normal(std=0.02)\n",
    "b_init      = lasagne.init.Constant(val=0.0)\n",
    "g_init      = lasagne.init.Normal(mean=1.,std=0.02)\n",
    "\n",
    "def sharedX(X, dtype=theano.config.floatX, name=None):\n",
    "    return theano.shared(np.asarray(X, dtype=dtype), name=name)\n",
    "\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "srng = RandomStreams(seed=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(zx, depth):\n",
    "    return (zx - 1)*2**depth + 1\n",
    "\n",
    "def sample_z(config, batch_size, zx):\n",
    "    Z = np.zeros( (batch_size, config.nz, zx, zx) )\n",
    "    Z[:, config.nz_global:config.nz_global+config.nz_local] = np.random.uniform(-1.,1., (batch_size, config.nz_local, zx, zx) )\n",
    "    Z[:,:config.nz_global] = np.random.uniform(-1.,1., (batch_size, config.nz_global, 1, 1) )\n",
    "    return Z\n",
    "\n",
    "def get_data(dir_data: str, batch_size: int, crop_size: int):\n",
    "    \"\"\" random cropping samples from image in the directory\n",
    "    inputs:\n",
    "           - dir_data (str) : path to users image directory\n",
    "           - batch_size (int) : size of dataaset to prepare\n",
    "           - crop_size(int) : dimension of cropped image          \n",
    "    \"\"\"\n",
    "    img = np.array( Image.open( dir_data) )\n",
    "    w, h = img.shape[:2]\n",
    "    while True: \n",
    "        batch = np.zeros( (batch_size, 3, crop_size, crop_size))\n",
    "        for i in range(batch_size):\n",
    "            rdm_x = random.randrange(0, w - crop_size, 1)\n",
    "            rdm_y = random.randrange(0, h - crop_size, 1)\n",
    "            sample = np.copy( img[rdm_x:rdm_x+crop_size, rdm_y:rdm_y+crop_size] )\n",
    "            \n",
    "            batch[i, 0, ...] = np.copy(sample[..., 0])\n",
    "            batch[i, 1, ...] = np.copy(sample[..., 1])\n",
    "            batch[i, 2, ...] = np.copy(sample[..., 2])\n",
    "\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    lr          = 0.0002                \n",
    "    b1          = 0.5                   \n",
    "    l2_fac      = 1e-8                  \n",
    "    epoch_count = 100                   \n",
    "    k           = 1                     # number of D updates vs G updates\n",
    "    batch_size  = 25\n",
    "    epoch_iters = batch_size * 500\n",
    "                 \n",
    "    def __init__(self):    \n",
    "        # sampling\n",
    "        self.nz_local = 30    \n",
    "        self.nz_global = 15                 \n",
    "        self.nz_periodic = 0                  \n",
    "        self.nz_periodic_MLPnodes = 50\n",
    "        self.nz          = self.nz_local+self.nz_global+self.nz_periodic*2\n",
    "        self.periodic_affine = False            \n",
    "        self.zx          = 6                    # number of spatial dimensions in Z\n",
    "        self.zx_sample   = 32                   # size of the spatial dimension in Z for producing the samples    \n",
    "\n",
    "        # network\n",
    "        self.nc          = 3                     # number of channels in input X (i.e. r,g,b)\n",
    "        self.gen_ks      = ([(5,5)] * 5)[::-1]   # kernel sizes for each layer for G\n",
    "        self.dis_ks      = [(5,5)] * 5           # kernel sizes on each layer for D\n",
    "        self.gen_ls      = len(self.gen_ks)           # number of layers for G\n",
    "        self.dis_ls      = len(self.dis_ks)           # number of layers for D\n",
    "        self.gen_fn      = [self.nc]+[2**(n+6) for n in range(self.gen_ls-1)]  # number of filters for G\n",
    "        self.gen_fn      = self.gen_fn[::-1]\n",
    "        self.dis_fn      = [self.nc] + [2**(n+6) for n in range(self.dis_ls-1)]  # number of filters for D\n",
    "        self.npx         = upsample(self.zx, self.gen_ls) # shape of output\n",
    "        \n",
    "        ## directory\n",
    "        self.save_name   = \"trained network\"\n",
    "        self.load_name   = None # if None, train from scratch \n",
    "        self.dir_data    = \"/path/to/directory/img.jpg\"\n",
    "           \n",
    "    def data_iter(self):\n",
    "        return get_data(self.dir_data, self.batch_size, self.npx)\n",
    "\n",
    "    def __str__(self): # print_info\n",
    "        return_str = f\"Learning and generating samples from zx {self.zx}, which yields images of size npx {upsample(self.zx, self.gen_ls)}\\n\"\n",
    "        return_str += f\"Generator: {self.gen_fn}\\n\"\n",
    "        return_str += f\"Discriminator: {self.dis_fn}\\n\"\n",
    "        return_str += f\"Saving samples and model data to file {self.save_name}\"\n",
    "        return return_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicLayer(lasagne.layers.Layer):\n",
    "    def __init__(self,incoming,config,wave_params):\n",
    "        self.config = config       \n",
    "        self.wave_params = wave_params\n",
    "        self.input_layer= incoming\n",
    "        self.input_shape = incoming.output_shape\n",
    "        self.get_output_kwargs = []\n",
    "        self.params = {}\n",
    "        for p in wave_params:\n",
    "            self.params[p] = set('trainable')\n",
    "            \n",
    "    def _wave_calculation(self,Z):\n",
    "        if self.config.nz_periodic ==0:\n",
    "            return Z\n",
    "        nPeriodic = self.config.nz_periodic\n",
    "\n",
    "        if self.config.nz_global > 0:  # #MLP or directly a weight vector in case of no Global dims\n",
    "            h = T.tensordot(Z[:, :self.config.nz_global], self.wave_params[0], [1, 0]).dimshuffle(0, 3, 1, 2) + self.wave_params[1].dimshuffle('x', 0, 'x', 'x')\n",
    "            band0 = (T.tensordot(relu(h),self.wave_params[2], [1, 0]).dimshuffle(0, 3, 1, 2)) + self.wave_params[3].dimshuffle('x', 0, 'x', 'x')  # #moved relu inside\n",
    "        else:\n",
    "            band0 = self.wave_params[0].dimshuffle('x', 0, 'x', 'x')\n",
    "        \n",
    "        if self.config.periodic_affine:\n",
    "            band1 = Z[:, -nPeriodic * 2::2] * band0[:, :nPeriodic] + Z[:, -nPeriodic * 2 + 1::2] * band0[:, nPeriodic:2 * nPeriodic]\n",
    "            band2 = Z[:, -nPeriodic * 2::2] * band0[:, 2 * nPeriodic:3 * nPeriodic] + Z[:, -nPeriodic * 2 + 1::2] * band0[:, 3 * nPeriodic:]\n",
    "        else:\n",
    "            band1 = Z[:, -nPeriodic * 2::2] * band0[:, :nPeriodic] \n",
    "            band2 = Z[:, -nPeriodic * 2 + 1::2] * band0[:, 3 * nPeriodic:]\n",
    "        band = T.concatenate([band1 , band2], axis=1)       \n",
    "        \n",
    "        band += srng.uniform((Z.shape[0],nPeriodic * 2)).dimshuffle(0,1, 'x', 'x') *np.pi*2\n",
    "        return T.concatenate([Z[:, :-2 * nPeriodic], T.sin(band)], axis=1)\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return self._wave_calculation(input)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0],input_shape[1]+self.config.nz_periodic*2,input_shape[2],input_shape[3])     \n",
    "\n",
    "periodic = lambda incoming,config,wave_params: PeriodicLayer(incoming,config,wave_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, load=None):\n",
    "        \"\"\"\n",
    "        :param load: (str) directory to stored model\n",
    "        \"\"\"\n",
    "        if load is not None:\n",
    "            print( f\"loading trained model from {load}\")\n",
    "            vals =joblib.load(load)\n",
    "            self.config = vals[\"config\"]                       \n",
    "            self.dis_W = [sharedX(p) for p in vals[\"dis_W\"]]\n",
    "            self.dis_g = [sharedX(p) for p in vals[\"dis_g\"]]\n",
    "            self.dis_b = [sharedX(p) for p in vals[\"dis_b\"]]\n",
    "            self.gen_W = [sharedX(p) for p in vals[\"gen_W\"]]\n",
    "            self.gen_g = [sharedX(p) for p in vals[\"gen_g\"]]\n",
    "            self.gen_b = [sharedX(p) for p in vals[\"gen_b\"]]\n",
    "            self.wave_params = [sharedX(p) for p in vals[\"wave_params\"]]\n",
    "            \n",
    "            self.config.gen_ks = []\n",
    "            self.config.gen_fn = []\n",
    "            \n",
    "            l = len(vals[\"gen_W\"])\n",
    "            for i in range(l):\n",
    "                if i==0:\n",
    "                    self.config.nz = vals[\"gen_W\"][i].shape[0]\n",
    "                else:\n",
    "                    self.config.gen_fn +=[vals[\"gen_W\"][i].shape[0]]\n",
    "                self.config.gen_ks += [(vals[\"gen_W\"][i].shape[2],vals[\"gen_W\"][i].shape[3])]\n",
    "            self.config.nc = vals[\"gen_W\"][i].shape[1]\n",
    "            self.config.gen_fn +=[self.config.nc]\n",
    "            self.config.dis_ks = []\n",
    "            self.config.dis_fn = []\n",
    "            \n",
    "            l = len(vals[\"dis_W\"])\n",
    "            for i in range(l):\n",
    "                self.config.dis_fn +=[vals[\"dis_W\"][i].shape[1]]   \n",
    "                self.config.dis_ks += [(vals[\"gen_W\"][i].shape[2],vals[\"gen_W\"][i].shape[3])]             \n",
    "\n",
    "            self._setup_gen_params(self.config.gen_ks, self.config.gen_fn)\n",
    "            self._setup_dis_params(self.config.dis_ks, self.config.dis_fn)\n",
    "        else:\n",
    "            self.config = Config()\n",
    "            self._setup_gen_params(self.config.gen_ks, self.config.gen_fn)\n",
    "            self._setup_dis_params(self.config.dis_ks, self.config.dis_fn)\n",
    "            self._sample_initials()\n",
    "            self._setup_wave_params()\n",
    "            \n",
    "        self._build_sgan()\n",
    "    \n",
    "    def save(self, dir_save):\n",
    "        print( f\"saving trained model at {dir_save}\" )\n",
    "        \n",
    "        vals = {}\n",
    "        vals[\"config\"] = self.config\n",
    "        vals[\"dis_W\"] = [p.get_value() for p in self.dis_W]\n",
    "        vals[\"dis_g\"] = [p.get_value() for p in self.dis_g]\n",
    "        vals[\"dis_b\"] = [p.get_value() for p in self.dis_b]\n",
    "        vals[\"gen_W\"] = [p.get_value() for p in self.gen_W]\n",
    "        vals[\"gen_g\"] = [p.get_value() for p in self.gen_g]\n",
    "        vals[\"gen_b\"] = [p.get_value() for p in self.gen_b]\n",
    "        vals[\"wave_params\"] = [p.get_value() for p in self.wave_params]\n",
    "        vals[\"m\"] = [p.get_value() for p in self.bm]\n",
    "        vals[\"istd\"] = [p.get_value() for p in self.bi]\n",
    "        joblib.dump(vals, self.config.save_name, True)\n",
    "\n",
    "    \n",
    "    def _setup_wave_params(self):\n",
    "        if self.config.nz_periodic:\n",
    "            nPeriodic = self.config.nz_periodic\n",
    "            nperiodK = self.config.nz_periodic_MLPnodes\n",
    "            if self.config.nz_global > 0 and nperiodK > 0: \n",
    "                lin1 =  sharedX( g_init.sample( (self.config.nz_global,nperiodK)))\n",
    "                bias1 = sharedX( g_init.sample( (nperiodK)))\n",
    "                lin2 =  sharedX( g_init.sample( (nperiodK,nPeriodic * 2*2)))\n",
    "                bias2 = sharedX( g_init.sample( (nPeriodic * 2*2)))\n",
    "                self.wave_params = [lin1,bias1,lin2,bias2]\n",
    "            else: ##in case no global dimensions learn global wave numbers\n",
    "                bias2 = sharedX( g_init.sample( (nPeriodic * 2*2)))\n",
    "                self.wave_params = [bias2]\n",
    "            a = np.zeros(nPeriodic * 2*2)              \n",
    "            a[:nPeriodic]=1#x\n",
    "            a[nPeriodic:2*nPeriodic]=0#y\n",
    "            a[2*nPeriodic:3*nPeriodic]=0#x\n",
    "            a[3*nPeriodic:]=1#y\n",
    "            self.wave_params[-1].set_value(np.float32(a)) \n",
    "        else:\n",
    "            self.wave_params = []\n",
    "\n",
    "    def _setup_gen_params(self, gen_ks, gen_fn):\n",
    "        self.gen_ks = [(5,5)] * 5 if gen_ks==None else gen_ks\n",
    "        self.gen_depth = len(self.gen_ks)\n",
    "        \n",
    "        if gen_fn!=None:\n",
    "            assert len(gen_fn)==len(self.gen_ks), 'Layer number of filter numbers and sizes does not match.'\n",
    "            self.gen_fn = gen_fn\n",
    "        else:\n",
    "            self.gen_fn = [64] * self.gen_depth\n",
    "    \n",
    "\n",
    "    def _setup_dis_params(self, dis_ks, dis_fn):\n",
    "        self.dis_ks = [(5,5)] *5 if dis_ks==None else dis_ks\n",
    "        self.dis_depth = len(dis_ks)\n",
    "\n",
    "        if dis_fn!=None:\n",
    "            assert len(dis_fn)==len(self.dis_ks), 'Layer number of filter numbers and sizes does not match.'\n",
    "            self.dis_fn = dis_fn\n",
    "        else:\n",
    "            self.dis_fn = [64] * self.dis_depth\n",
    "\n",
    "    def _sample_initials(self):\n",
    "        self.dis_W, self.dis_b, self.dis_g = [], [], []\n",
    "        self.dis_W.append( sharedX( w_init.sample( (self.dis_fn[0], self.config.nc, self.dis_ks[0][0], self.dis_ks[0][1]) )) )\n",
    "        \n",
    "        for l in range(self.dis_depth-1):\n",
    "            self.dis_W.append( sharedX( w_init.sample( (self.dis_fn[l+1], self.dis_fn[l], self.dis_ks[l+1][0], self.dis_ks[l+1][1]) ) ) )\n",
    "            self.dis_b.append( sharedX( b_init.sample( (self.dis_fn[l+1]) ) ) )\n",
    "            self.dis_g.append( sharedX( g_init.sample( (self.dis_fn[l+1]) ) ) )\n",
    "    \n",
    "        self.gen_b, self.gen_g = [], []\n",
    "        for l in range(self.gen_depth-1):\n",
    "            self.gen_b += [sharedX( b_init.sample( (self.gen_fn[l]) ) ) ]\n",
    "            self.gen_g += [sharedX( g_init.sample( (self.gen_fn[l]) ) ) ]\n",
    "\n",
    "        self.gen_W = []\n",
    "        last = self.config.nz\n",
    "        for l in range(self.gen_depth-1):\n",
    "            self.gen_W +=[sharedX( w_init.sample((last,self.gen_fn[l], self.gen_ks[l][0],self.gen_ks[l][1])))]\n",
    "            last=self.gen_fn[l]\n",
    "        self.gen_W +=[sharedX( w_init.sample((last,self.gen_fn[-1], self.gen_ks[-1][0],self.gen_ks[-1][1])))]   \n",
    "\n",
    "    def _spatial_generator(self, inlayer):\n",
    "        layers  = [inlayer]\n",
    "        layers.append(periodic(inlayer,self.config,self.wave_params))\n",
    "        \n",
    "        m, i =[], []\n",
    "        for l in range(self.gen_depth-1):\n",
    "            layers.append( batchnorm(tconv(layers[-1], self.gen_fn[l], self.gen_ks[l],self.gen_W[l],\\\n",
    "                                           nonlinearity=relu),gamma=self.gen_g[l],beta=self.gen_b[l],alpha=1.0) )\n",
    "            m +=[layers[-1].input_layer.mean]\n",
    "            i +=[layers[-1].input_layer.inv_std]\n",
    "        output  = tconv(layers[-1], self.gen_fn[-1], self.gen_ks[-1],self.gen_W[-1] , nonlinearity=tanh)\n",
    "        return output,m,i\n",
    "    \n",
    "    def _spatial_generator_det(self, inlayer):\n",
    "        layers  = [inlayer]\n",
    "        layers.append(periodic(inlayer,self.config,self.wave_params))\n",
    "        for l in range(self.gen_depth-1):\n",
    "            layers.append( batchnorm(tconv(layers[-1], self.gen_fn[l], self.gen_ks[l],self.gen_W[l], nonlinearity=relu),gamma=self.gen_g[l],\\\n",
    "                                  beta=self.gen_b[l],mean=self.im[l],inv_std=self.im[l+self.gen_depth-1]) )\n",
    "        output  = tconv(layers[-1], self.gen_fn[-1], self.gen_ks[-1],self.gen_W[-1] , nonlinearity=tanh)\n",
    "        return output\n",
    "    \n",
    "    def _spatial_discriminator(self, inlayer):\n",
    "        layers  = [inlayer]\n",
    "        layers.append( conv(layers[-1], self.dis_fn[0], self.dis_ks[0], self.dis_W[0], None, nonlinearity=lrelu) )\n",
    "        for l in range(1,self.dis_depth-1):\n",
    "            layers.append( batchnorm(conv(layers[-1], self.dis_fn[l], self.dis_ks[l], self.dis_W[l],None,nonlinearity=lrelu),gamma=self.dis_g[l-1],beta=self.dis_b[l-1]) )\n",
    "        output = conv(layers[-1], self.dis_fn[-1], self.dis_ks[-1], self.dis_W[-1], None, nonlinearity=sigmoid)\n",
    "\n",
    "        return output   \n",
    "    \n",
    "    def _build_sgan(self):    \n",
    "        Z               = lasagne.layers.InputLayer((None,self.config.nz,None,None))   \n",
    "        X               = lasagne.layers.InputLayer((self.config.batch_size,self.config.nc,self.config.npx,self.config.npx))\n",
    "        self.forDebug = Z\n",
    "        gen_X,i,m           = self._spatial_generator(Z)\n",
    "        self.im = i+m \n",
    "\n",
    "        gen_X_det           = self._spatial_generator_det(Z)\n",
    "        d_real          = self._spatial_discriminator(X)\n",
    "        d_fake          = self._spatial_discriminator(gen_X)\n",
    "\n",
    "        prediction_gen  = lasagne.layers.get_output(gen_X)\n",
    "        prediction_gen_det  = lasagne.layers.get_output(gen_X_det,deterministic=True)\n",
    "        prediction_real = lasagne.layers.get_output(d_real)\n",
    "        prediction_fake = lasagne.layers.get_output(d_fake)\n",
    "\n",
    "        params_g        = lasagne.layers.get_all_params(gen_X, trainable=True)\n",
    "        params_d        = lasagne.layers.get_all_params(d_real, trainable=True)\n",
    "\n",
    "        l2_gen          = lasagne.regularization.regularize_network_params(gen_X, lasagne.regularization.l2)\n",
    "        l2_dis          = lasagne.regularization.regularize_network_params(d_real, lasagne.regularization.l2)\n",
    "\n",
    "        \n",
    "        obj_d= -T.mean(T.log(1-prediction_fake)) - T.mean( T.log(prediction_real)) + self.config.l2_fac * l2_dis\n",
    "        obj_g= -T.mean(T.log(prediction_fake)) + self.config.l2_fac * l2_gen\n",
    "\n",
    "        updates_d       = lasagne.updates.adam(obj_d, params_d, self.config.lr, self.config.b1)\n",
    "        updates_g       = lasagne.updates.adam(obj_g, params_g, self.config.lr, self.config.b1)\n",
    "\n",
    "        st = time.time() \n",
    "        self.train_d    = theano.function([X.input_var, Z.input_var], obj_d, updates=updates_d, allow_input_downcast=True)\n",
    "        print( f\"Compiling Discriminator {round( time.time() - st, 4 )}s\" )\n",
    "        \n",
    "        st = time.time()\n",
    "        self.train_g    = theano.function([Z.input_var], obj_g, updates=updates_g, allow_input_downcast=True)\n",
    "        print( f\"Compiling Generator {round( time.time() - st, 4 )}s\" )\n",
    "        \n",
    "        st = time.time()\n",
    "        self.generate   = theano.function([Z.input_var], prediction_gen, allow_input_downcast=True)\n",
    "        self.generate_det   = theano.function([Z.input_var], prediction_gen_det, allow_input_downcast=True)\n",
    "        print( f\"Compiling rest {round( time.time() - st, 4 )}s\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN()\n",
    "print( gan.config )\n",
    "c = gan.config\n",
    "\n",
    "epoch = 0\n",
    "total_iters = 0\n",
    "c.epoch_count = 10\n",
    "while epoch < c.epoch_count: \n",
    "    epoch += 1\n",
    "    print( f\"epoch {epoch}\" )\n",
    "    \n",
    "    Gcost = []\n",
    "    Dcost = []\n",
    "    n_iters = c.epoch_iters / c.batch_size\n",
    "    for i_iters, samples in enumerate( tqdm(c.data_iter(), total=n_iters) ): \n",
    "        if i_iters >= n_iters:\n",
    "            break\n",
    "        total_iters += 1\n",
    "        \n",
    "        zs = sample_z(c, c.batch_size, c.zx)\n",
    "        if total_iters % (c.k+1) == 0: \n",
    "            cost = gan.train_g(zs)\n",
    "            Gcost.append(cost)\n",
    "        else: \n",
    "            cost = gan.train_d(samples, zs)\n",
    "            Dcost.append(cost)\n",
    "        \n",
    "    print( f\"Epoch {epoch} - G ({np.mean(Gcost)}), D ({np.mean(Dcost)})\" )    \n",
    "    outs = gan.generate(z[:3])\n",
    "    outs = outs.transpose( (0,2,3,1) )\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot( 1, 3, 1 )\n",
    "    plt.imshow( outs[0] )\n",
    "    plt.axis('off')\n",
    "    plt.subplot( 1, 3, 2 )\n",
    "    plt.imshow( outs[1] )\n",
    "    plt.axis('off')\n",
    "    plt.subplot( 1, 3, 3 )\n",
    "    plt.imshow( outs[2] )\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.save() # save trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
